version: '3.4'
services:
  hadoop: 
      image: nikammoz/hadoop:3.3.1
      build: ./hadoop
      hostname: hadoop
      container_name: hadoop
      ports:
        - 50070:50070
        - 50075:50075
        - 8088:8088
        - 8188:8188
        - 9000:9000
        - 9870:9870
      networks:
        - services
      volumes:
        - dfs_data:/home/hduser/dfs/data

  spark-master:
      image: bde2020/spark-master:3.1.1-hadoop3.2-java11
      container_name: spark-master
      hostname: spark-master
      ports:
        - 8080:8080
        - 7077:7077
      environment:
        - CORE_CONF_fs_defaultFS=hdfs://hadoop:8020
      env_file:
        - ./hadoop.env
      networks:
        - services
       
  spark-worker:
      image: bde2020/spark-worker:3.1.1-hadoop3.2-java11
      container_name: spark-worker
      hostname: spark-worker
      depends_on:
        - spark-master
      environment:
        - SPARK_MASTER=spark://spark-master:7077
        - CORE_CONF_fs_defaultFS=hdfs://hadoop:8020
        # - HIVE_CORE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore/metastore
      depends_on:
        - spark-master
      ports:
        - 8081:8081
      env_file:
        - ./hadoop.env
      networks:
        - services
  pyspark:
    image: jupyter/all-spark-notebook:hadoop-3.2
    container_name: pyspark
    hostname: pyspark
    # volumes:
    #   - c:/code/pyspark-data:/home/jovyan
    ports:
      - 8888:8888

volumes:
  dfs_data:

networks: 
  services:
    name: ${APP_NAME}_network
